{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ThrupthiAnn/SummerSchool2021_HandsOn_Aug7/blob/main/CNN/2_CNNs_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKcw3bNmJnws"
   },
   "source": [
    "## Training CNN on CIFAR10 Dataset [1]\n",
    "\n",
    "We begin with building a CNN architecture for image classification task on CIFAR10 dataset. In this first part of the tutorial, we will understand how to arrange the different architectural components of CNN network, defining the appropriate loss, training the network using backpropagation and finally testing it on the test data.\n",
    "\n",
    "To make data loading simple, we would use the torchvision package created as part of PyTorch which has data loaders for standard datasets such as ImageNet, CIFAR10, MNIST.\n",
    "\n",
    "### CIFAR10 dataset\n",
    "![CIFAR10](https://github.com/ckraju/summer/blob/main/data/cnn-basics/cifar10.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LkwX_yLJnww"
   },
   "source": [
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qw_iRUBNJnwx"
   },
   "outputs": [],
   "source": [
    "#a Tensor library with GPU support\n",
    "import torch\n",
    "\n",
    "#Datasets, Transforms and Models specific to Computer Vision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#differentiation library that supports all differentiable Tensor operations in torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#a neural networks library integrated with autograd functionality\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#an optimization package with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.\n",
    "import torch.optim as optim\n",
    "\n",
    "#scientific computing library for Python\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "#plotting and visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "#Display on the notebook\n",
    "%matplotlib inline \n",
    "plt.ion() #Turn interactive mode on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "m54ctMINLdKX"
   },
   "outputs": [],
   "source": [
    "rootfolder = '/CVSummerSchool2021/CNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOnZ_2k-Jnwy"
   },
   "source": [
    "### Dataloader and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "referenced_widgets": [
      "093fbceab84c44189f631a2b4f93d645",
      "191d4d1f9b834e178740b9d96525b1e6",
      "7935fe2bc3a8418e909b427259bb8a2b",
      "50ea7dedad4e45b6b0113b5e61165a23",
      "203ea95085b8410082cbeb548a8720fd",
      "7bf2cb636d9d4d919b98d9f09e7da1ad",
      "8428ff2a0e5f48dab994e3f5edfb2889",
      "3a95f33879624c5ea7216d06bf61eafc"
     ]
    },
    "id": "q5E-t1nyJnwz",
    "outputId": "380b5684-1f3b-4671-ef82-02b6b66ac9fd"
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/CVSummerSchool2021'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c0811ad06022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#inbuilt dataset class for reading CIFAR10 dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m trainset = torchvision.datasets.CIFAR10(root=os.path.join(rootfolder, 'imgFolders'), train=True,\n\u001b[0;32m----> 9\u001b[0;31m                                         download=True, transform=transform)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#dataloader for Batching, shuffling and loading data in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# check if file is already present locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/CVSummerSchool2021'"
     ]
    }
   ],
   "source": [
    "####train data\n",
    "#Compose transforms (applies data transformation and augmentation) prior to feeding to training\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#inbuilt dataset class for reading CIFAR10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root=os.path.join(rootfolder, 'imgFolders'), train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#dataloader for Batching, shuffling and loading data in parallel\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "####test data\n",
    "testset = torchvision.datasets.CIFAR10(root=os.path.join(rootfolder, 'imgFolders'), train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJIbPCTJJnw0"
   },
   "source": [
    "#### Visualizing the dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "wifiT_scJnw1",
    "outputId": "61acdd1f-c593-44ae-d7f8-e2137d3d91ef"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-06932ff7bc1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNdagfY5Jnw4"
   },
   "source": [
    "### Defining the CNN architecture\n",
    "![cnn-arch](https://github.com/ckraju/summer/blob/main/data/cnn-basics/cnn-arch.png?raw=1)\n",
    "#### Basic Convolutional Layer\n",
    "![conv](https://github.com/ckraju/summer/blob/main/data/cnn-basics/conv-all.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIIXBMDlJnw5"
   },
   "source": [
    "To create a network, we should first inhert the base class nn.Module. You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMHXc0BuJnw6",
    "outputId": "1f9aad1f-077a-4822-bd99-6e03be6004a8"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    #define the learnable paramters by calling the respective modules (nn.Conv2d, nn.MaxPool2d etc.)\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #calling conv2d module for convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5,stride=1,padding=0,bias=True)\n",
    "        \n",
    "        #calling MaxPool2d module for max pooling with downsampling of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        #fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    #defining the structure of the network\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Applying relu activation after each conv layer\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        #reshaping to 1d for giving input to fully connected units\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "#Printing the network architecture\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dciWjVsJJnw6",
    "outputId": "2b18d81e-2339-4586-e37d-8cfccf01fe9c"
   },
   "outputs": [],
   "source": [
    "#Printing the parameter values\n",
    "params = list(net.parameters())\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbrbq7mpJnw7"
   },
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWwV4iZhJnw7",
    "outputId": "a680c848-4717-49d4-e87e-ced1570aa009"
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(1, 3, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afKAHYK8Jnw8"
   },
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eemfnrwtJnw9"
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O43ISrlPJnw9"
   },
   "source": [
    "#### Loss Function\n",
    "In this example, we will use Classification Cross-Entropy loss and SGD with momentum.<br>\n",
    "Cross Entropy loss is given as:- $L=-\\sum_i y_i \\log(p_i)$ and $p_i=\\frac{\\exp^{x_i}}{\\sum_k \\exp^{x_k}}$\n",
    "\n",
    "There are many other loss functions such as MSELoss, L1Loss etc. Visit [here](http://pytorch.org/docs/master/nn.html#loss-functions) for other loss functions.\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "$$w_{n+1} = w_{n} - \\eta \\triangle$$\n",
    "$$\\triangle = 0.9\\triangle + \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "Although SGD is the most popular and basic optimizer that one should first try. There are many adaptive optimizers like Adagrad,Adadelta RMSProp and many more. Visit [here](http://pytorch.org/docs/master/optim.html) for other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ7yhenMJnw9"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6xhQsEwJnw9"
   },
   "source": [
    "#### Training in mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRcyOGsYJnw9",
    "outputId": "48ff5c2f-5eb3-4236-ab8c-15f43eb84aa1"
   },
   "outputs": [],
   "source": [
    "plotIter = 2000\n",
    "plotIterCntr = 0\n",
    "numEpochs = 1\n",
    "trainLoss = np.zeros((plotIter*numEpochs,1))\n",
    "trainIter = np.arange(plotIter*numEpochs)\n",
    "\n",
    "#Transferring the model to GPU\n",
    "net.cuda()\n",
    "\n",
    "for epoch in range(numEpochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % plotIter == plotIter-1:    # print every plotIter mini-batches\n",
    "            trainLoss[plotIterCntr] = running_loss / plotIter\n",
    "            plotIterCntr+=1\n",
    "            \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / plotIter))\n",
    "            running_loss = 0.0\n",
    " \n",
    "#Plotting the training graph\n",
    "plt.plot(np.arange(plotIterCntr)*plotIter,trainLoss[0:plotIterCntr], label=\"train\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "WrON9pzQJnw-"
   },
   "source": [
    "#### Forward Pass over the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrQGF4YeJnw-",
    "outputId": "22bf8a8e-e980-4162-9e0b-5b3e45de031d"
   },
   "outputs": [],
   "source": [
    "outputs = net(Variable(images).cuda())\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print('Predicted: ', ' '.join('%5s' % classes[predicted[j] for j in range(4)]))\n",
    "print('Predicted: ')\n",
    "for i in range(4):\n",
    "  print(classes[predicted[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXbq_d1tJnw_"
   },
   "source": [
    "#### Test Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94qHAWkHJnw_",
    "outputId": "72fc4a1d-ab84-4a72-aa2b-7ea7dd7bae87"
   },
   "outputs": [],
   "source": [
    "## Prediction of accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#Iterating over the batches returned from testloader\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images.cuda()))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    labels = labels.cuda()\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oPdhJupJnxA"
   },
   "source": [
    "#### Hyperparameters -- Learning Rate\n",
    "![lR](https://github.com/ckraju/summer/blob/main/data/cnn-basics/learning-rate.png?raw=1)\n",
    "Typically learning rates are set using step decay, exponential/log space decay and manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzu4giXLJnxA"
   },
   "source": [
    "##### Self Exercise -1\n",
    "- Manipulate the learning rates in range of 10-0.0001, momentum from (0.5,0.9) and understand how the training proceeds.\n",
    "- Check the performance by changing:-\n",
    "    - Add two more conv layers and check the training performance.\n",
    "    - Optimizer to Adagrad and check the performance.\n",
    "    - Modify the loss function to MSELoss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjPMXqJ5JnxA"
   },
   "source": [
    "### Visualization of Layers\n",
    "To undestand what is being learnt in the filters, we now visualize the weights. Here we restrict ourselves to conv1 layer which contains 3d filters and can be visualized in the image space itself. For visualizing layers other that first one, read [3] and the follow up papers taught in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "id": "u1KKwAqsJnxA",
    "outputId": "3cab706f-0f50-48be-f6d0-59be5b119b1b"
   },
   "outputs": [],
   "source": [
    "#### Visualization of Layer 1 weights\n",
    "def plot_kernels(tensor, num_cols=6):\n",
    "    if not tensor.ndim==4:\n",
    "        raise Exception(\"assumes a 4D tensor\")\n",
    "    if not tensor.shape[-1]==3:\n",
    "        raise Exception(\"last dim needs to be 3 to plot\")\n",
    "    num_kernels = tensor.shape[0]\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    for i in range(tensor.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        ax1.imshow(tensor[i])\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "net_cpu = net.cpu()\n",
    "\n",
    "#Reading the conv1 weights\n",
    "dat = net_cpu.conv1.weight.data.numpy()\n",
    "\n",
    "#Normalizing the weights from (0,1)\n",
    "maxVal = dat.max()\n",
    "minVal = abs(dat.min())\n",
    "maxVal = max(maxVal,minVal)\n",
    "dat = dat / maxVal\n",
    "dat = dat / 2\n",
    "dat = dat + 0.5\n",
    "\n",
    "#calling visualization function\n",
    "plot_kernels(dat.transpose((0,2,3,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULSFbOTUJnxB"
   },
   "source": [
    "#### Generalization using Dropouts\n",
    "Dropouts perform a stochastic regularization by dropping out hidden units randomly with fixed probability $p$ while training and while testing the all units are preserved but scaled with $p$.\n",
    "![dropouts](https://github.com/ckraju/summer/blob/main/data/cnn-basics/dropout.png?raw=1)\n",
    "\n",
    "#### Batch Normalization \n",
    "Normalize the output feature maps across samples in a mini-batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ja1v7DYwJnxB"
   },
   "outputs": [],
   "source": [
    "class Net_DB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_DB, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        \n",
    "        #applying batch normalization\n",
    "        self.conv1_bn = nn.BatchNorm2d(num_features=6)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        #Flag to apply droputs only at training\n",
    "        self.is_training = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(self.conv1_bn(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Applying dropouts\n",
    "        x = F.dropout(x, p=0.2, training=self.is_training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net_db = Net_DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "eVEmyAujJnxB",
    "outputId": "e5714497-e8c6-454c-d4e2-cce5e20e2a21"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_db.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "plotIter = 2000\n",
    "plotIterCntr = 0\n",
    "numEpochs = 1\n",
    "trainLoss = np.zeros((plotIter*numEpochs,1))\n",
    "trainIter = np.arange(plotIter*numEpochs)\n",
    "\n",
    "net_db.cuda()\n",
    "net_db.is_training = True\n",
    "\n",
    "for epoch in range(numEpochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net_db(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % plotIter == plotIter-1:    # print every plotIter mini-batches\n",
    "            trainLoss[plotIterCntr] = running_loss / plotIter\n",
    "            plotIterCntr+=1\n",
    "            \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / plotIter))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "plt.plot(np.arange(plotIterCntr)*plotIter,trainLoss[0:plotIterCntr], label=\"train\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print('Finished Training with dropouts and batch normalization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe7x_917JnxC",
    "outputId": "22ec7739-17ee-498d-bd71-f8bf3a115458"
   },
   "outputs": [],
   "source": [
    "net_db.is_training = False\n",
    "\n",
    "## Prediction of accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net_db(Variable(images.cuda()))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    labels = labels.cuda()\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xfmTKubJnxD"
   },
   "source": [
    "### References\n",
    "[1] Soumith Chintala, [Deep Learning with PyTorch: A 60 Minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)<br>\n",
    "[2] [PyTorch documentation](http://pytorch.org/docs/)<br>\n",
    "[3] Zeiler and Fergus , Visualizing and Understanding Convolutional Networks, ECCV 2014<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "2_CNNs_Basics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "093fbceab84c44189f631a2b4f93d645": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7935fe2bc3a8418e909b427259bb8a2b",
       "IPY_MODEL_50ea7dedad4e45b6b0113b5e61165a23"
      ],
      "layout": "IPY_MODEL_191d4d1f9b834e178740b9d96525b1e6"
     }
    },
    "191d4d1f9b834e178740b9d96525b1e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "203ea95085b8410082cbeb548a8720fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3a95f33879624c5ea7216d06bf61eafc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50ea7dedad4e45b6b0113b5e61165a23": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a95f33879624c5ea7216d06bf61eafc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8428ff2a0e5f48dab994e3f5edfb2889",
      "value": " 170499072/? [00:11&lt;00:00, 15237150.48it/s]"
     }
    },
    "7935fe2bc3a8418e909b427259bb8a2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bf2cb636d9d4d919b98d9f09e7da1ad",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_203ea95085b8410082cbeb548a8720fd",
      "value": 170498071
     }
    },
    "7bf2cb636d9d4d919b98d9f09e7da1ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8428ff2a0e5f48dab994e3f5edfb2889": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
